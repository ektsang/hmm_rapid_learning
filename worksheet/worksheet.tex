\documentclass[11pt, oneside]{article}
\usepackage{geometry}
\geometry{letterpaper}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{parskip}

\renewcommand{\tabcolsep}{1cm}
\renewcommand{\arraystretch}{2}

\title{Hidden Markov Models}
\author{Rapid Learning Session 2014}
\date{}

\begin{document}
\maketitle

This is intended to be a short and informal introduction to HMMs.

\section{Introduction}
\subsection{What is a hidden Markov model?}
A \textbf{hidden Markov model} (HMM) is a common tool used to analyze sequential and time-based processes. As a very informal definition, the values that are generated from a standard Markov model are generated with probabilities that are dependent on the `state' of the model at any given time. At its core, this means there are two types of data we are primarily concerned with: a sequence of states, and a sequence of observations. In the case of a HMM, the observations are of course known, but the sequence of states is not (and therefore ``hidden''). Additionally, we are often interested in the parameters that describe how data is generated from the model and how the system switches back and forth between different states.

%\subsection{What kind of data is well modeled by a HMM?}
\subsection{Well-known examples of HMMs in biology}

\begin{itemize}
\item Gene finding (GENSCAN: \url{http://www.ncbi.nlm.nih.gov/pubmed/9149143}, \url{http://genes.mit.edu/GENSCAN.html})
\item Modeling protein sequences and homologs (HMMER: \url{http://hmmer.janelia.org/})
\item Chromatin state annotation (ChromHMM: \url{http://www.ncbi.nlm.nih.gov/pubmed/22373907}, \url{http://compbio.mit.edu/ChromHMM/})
\end{itemize}

\section{Algorithms}
\subsection{Formal Definition of a HMM}
\subsection{Interpretation}
\subsection{The Viterbi Algorithm}
\subsection{The Baum-Welch Algorithm}

This is the algorithm used to learn the parameters of an HMM given unlabeled training data. 
It is a special case of the expectation-maximization (EM) algorithm. These types of algorithms work by iterating over two steps: the E-step and the M-step.

In the case of HMMs, if we knew the state that each emission came from, then we could easily infer the emission and transition probabilities (we will do this in Exercise 1). However, in the case of unlabeled data, we do not have this information. The idea is therefore to initialize the parameters with some values then find the most likely state assignment (E-step). With that assignment in hand, we now have labeled data and can use maximum likelihood estimation to update our estimates for the emission and transition parameter values (M-step). We keep iterating between the E- and M-steps until convergence. It is important to note that we may not arrive at an optimal solution. Different initializations of the parameter values can lead to different results.

The inference of the most likely sequence of states (E-Step) is done with the forward-backward algorithm. The forward-backward algorithm returns a probability distribution over the states for each emission. If you want to use hard EM, you would select the single most likely state (i.e. the state with the highest assigned probability). A better idea is to use soft EM where you keep the soft assignments to states (i.e. the probability distribution).

\subsection{Caveats}

\section{Exercise 1}
Consider a simple two-state HMM that models the GC content of a DNA sequence. 
There is a state to represent GC-rich regions and another for regions that are not GC rich.
You have a sequence of observed emissions. 
And you also happen to know which state generated each emission (lucky you!). 
Given these data, you will estimate the parameters of the HMM. 

[Use starter code in exercise1.R]

\subsection{Estimate the emission probabilities for each of the two states}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
& A & T & G & C \\\hline
GC rich & & & &  \\\hline
not GC rich & & & & \\\hline
\end{tabular}
\end{table}

\subsection{Estimate the transition probabilities between states}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
& GC rich & not GC rich \\\hline
GC rich & &  \\\hline
not GC rich & &  \\\hline
\end{tabular}
\end{table}

\section{Exercise 2}
Consider an HMM of the same form as before. 
Again you observe a sequence of emissions. 
However, this time the states are also unknown. 
This time we will learn the parameters of the HMM without knowing the true sequence of states that generated the observations! 

[Use starter code in exercise2.R]

\subsection{Estimate the parameters of the HMM}
Emission probabilities
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
& A & T & G & C \\\hline
GC rich & & & &  \\\hline
not GC rich & & & & \\\hline
\end{tabular}
\end{table}

Transition probabilities
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
& GC rich & not GC rich \\\hline
GC rich & &  \\\hline
not GC rich & &  \\\hline
\end{tabular}
\end{table}

\subsection{Predict the most likely state sequence using the parameters you just estimated}

\end{document} 